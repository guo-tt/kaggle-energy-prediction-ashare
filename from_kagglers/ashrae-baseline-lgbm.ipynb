{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc, sys, warnings, random, math, psutil, pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "LOCAl_TEST = False\n",
    "seed_everything(SEED)\n",
    "TARGET = 'meter_reading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/ashrae-data-minification/train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-92e0bdb59d69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/ashrae-data-minification/train.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/ashrae-data-minification/test.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/kaggle/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m    144\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# 1) try standard libary Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/kaggle/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/ashrae-data-minification/train.pkl'"
     ]
    }
   ],
   "source": [
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('Load Data')\n",
    "train_df = pd.read_pickle('../input/ashrae-data-minification/train.pkl')\n",
    "test_df = pd.read_pickle('../input/ashrae-data-minification/test.pkl')\n",
    "\n",
    "building_df = pd.read_pickle('../input/ashrae-data-minification/building_metadata.pkl')\n",
    "\n",
    "train_weather_df = pd.read_pickle('../input/ashrae-data-minification/weather_train.pkl')\n",
    "test_weather_df = pd.read_pickle('../input/ashrae-data-minification/weather_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Building DF merge through concat \n",
    "#################################################################################\n",
    "# Benefits of concat:\n",
    "## Faster for huge datasets (columns number)\n",
    "## No dtype change for dataset\n",
    "## Consume less memmory \n",
    "\n",
    "temp_df = train_df[['building_id']]\n",
    "temp_df = temp_df.merge(building_df, on=['building_id'], how='left')\n",
    "del temp_df['building_id']\n",
    "train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "temp_df = test_df[['building_id']]\n",
    "temp_df = temp_df.merge(building_df, on=['building_id'], how='left')\n",
    "del temp_df['building_id']\n",
    "test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "\n",
    "del building_df, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Weather DF merge over concat (to not lose type)\n",
    "#################################################################################\n",
    "# Benefits of concat:\n",
    "## Faster for huge datasets (columns number)\n",
    "## No dtype change for dataset\n",
    "## Consume less memmory \n",
    "\n",
    "temp_df = train_df[['site_id','timestamp']]\n",
    "temp_df = temp_df.merge(train_weather_df, on=['site_id','timestamp'], how='left')\n",
    "del temp_df['site_id'], temp_df['timestamp']\n",
    "train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "temp_df = test_df[['site_id','timestamp']]\n",
    "temp_df = temp_df.merge(test_weather_df, on=['site_id','timestamp'], how='left')\n",
    "del temp_df['site_id'], temp_df['timestamp']\n",
    "test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "\n",
    "del train_weather_df, test_weather_df, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### Trick to use kernel hdd to store results\n",
    "#################################################################################\n",
    "\n",
    "# You can save just test_df or both if have sufficient space\n",
    "train_df.to_pickle('train_df.pkl')\n",
    "test_df.to_pickle('test_df.pkl')\n",
    "   \n",
    "del train_df, test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _ii:   817.0B\n",
      "                           _i6:   817.0B\n",
      "                           _i2:   715.0B\n",
      "                          _iii:   704.0B\n",
      "                           _i5:   704.0B\n",
      "                           _i4:   597.0B\n",
      "                           _i8:   421.0B\n",
      "                            _i:   368.0B\n",
      "                           _i7:   368.0B\n",
      "                           _oh:   240.0B\n",
      "Memory in Gb 0.3\n"
     ]
    }
   ],
   "source": [
    "########################### Check memory usage\n",
    "#################################################################################\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))\n",
    "print('Memory in Gb', get_memory_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'objective':'regression',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'rmse',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.05,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':800,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.00576\n",
      "[200]\ttraining's rmse: 0.865085\n",
      "[300]\ttraining's rmse: 0.796926\n",
      "[400]\ttraining's rmse: 0.757061\n",
      "[500]\ttraining's rmse: 0.729946\n",
      "[600]\ttraining's rmse: 0.706739\n",
      "[700]\ttraining's rmse: 0.68873\n",
      "[800]\ttraining's rmse: 0.67292\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.67292\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 0.99834\n",
      "[200]\ttraining's rmse: 0.860726\n",
      "[300]\ttraining's rmse: 0.799359\n",
      "[400]\ttraining's rmse: 0.760661\n",
      "[500]\ttraining's rmse: 0.733446\n",
      "[600]\ttraining's rmse: 0.709403\n",
      "[700]\ttraining's rmse: 0.691052\n",
      "[800]\ttraining's rmse: 0.674661\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.674661\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 0.999707\n",
      "[200]\ttraining's rmse: 0.85703\n",
      "[300]\ttraining's rmse: 0.795713\n",
      "[400]\ttraining's rmse: 0.75881\n",
      "[500]\ttraining's rmse: 0.732198\n",
      "[600]\ttraining's rmse: 0.710776\n",
      "[700]\ttraining's rmse: 0.69352\n",
      "[800]\ttraining's rmse: 0.675418\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.675418\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.00494\n",
      "[200]\ttraining's rmse: 0.859313\n",
      "[300]\ttraining's rmse: 0.794831\n",
      "[400]\ttraining's rmse: 0.7555\n",
      "[500]\ttraining's rmse: 0.728637\n",
      "[600]\ttraining's rmse: 0.707423\n",
      "[700]\ttraining's rmse: 0.687925\n",
      "[800]\ttraining's rmse: 0.672766\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.672766\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.01149\n",
      "[200]\ttraining's rmse: 0.871371\n",
      "[300]\ttraining's rmse: 0.808546\n",
      "[400]\ttraining's rmse: 0.767997\n",
      "[500]\ttraining's rmse: 0.738584\n",
      "[600]\ttraining's rmse: 0.710862\n",
      "[700]\ttraining's rmse: 0.690692\n",
      "[800]\ttraining's rmse: 0.673351\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.673351\n"
     ]
    }
   ],
   "source": [
    "########################### Model\n",
    "\n",
    "# Models saving\n",
    "model_filename = 'lgbm'\n",
    "models = []\n",
    "\n",
    "# Load train_df from hdd\n",
    "train_df = pd.read_pickle('train_df.pkl')\n",
    "\n",
    "remove_columns = ['timestamp',TARGET]\n",
    "features_columns = [col for col in list(train_df) if col not in remove_columns]\n",
    "\n",
    "if LOCAl_TEST:\n",
    "    tr_data = lgb.Dataset(train_df.iloc[:15000000][features_columns], label=np.log1p(train_df.iloc[:15000000][TARGET]))\n",
    "    vl_data = lgb.Dataset(train_df.iloc[15000000:][features_columns], label=np.log1p(train_df.iloc[15000000:][TARGET]))\n",
    "    eval_sets = [tr_data,vl_data]\n",
    "else:\n",
    "    tr_data = lgb.Dataset(train_df[features_columns], label=np.log1p(train_df[TARGET]))\n",
    "    eval_sets = [tr_data]\n",
    "\n",
    "# Remove train_df from hdd\n",
    "os.system('rm train_df.pkl')\n",
    "\n",
    "# Lets make 5 seeds mix model\n",
    "for cur_seed in [42,43,44,45,46]:\n",
    "    \n",
    "    # Seed everything\n",
    "    seed_everything(cur_seed)\n",
    "    lgb_params['seed'] = cur_seed\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "                lgb_params,\n",
    "                tr_data,\n",
    "                valid_sets = eval_sets,\n",
    "                verbose_eval = 100,\n",
    "            )\n",
    "\n",
    "    # For CV you may add fold number\n",
    "    # pickle.dump(estimator, open(model_filename + '__fold_' + str(i) + '.bin', \"wb\"))\n",
    "    pickle.dump(estimator, open(model_filename + '__seed_' + str(cur_seed)  + '.bin', 'wb'))\n",
    "    models.append(model_filename + '__seed_' + str(cur_seed)  + '.bin')\n",
    "\n",
    "if not LOCAl_TEST:\n",
    "    del tr_data, train_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for lgbm__seed_42.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for lgbm__seed_43.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for lgbm__seed_44.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for lgbm__seed_45.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for lgbm__seed_46.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "    meter_reading  row_id\n",
      "0        0.679761       0\n",
      "1        0.729967       1\n",
      "2        0.000000       2\n",
      "3        1.048019       3\n",
      "4        1.566109       4\n",
      "5        0.000000       5\n",
      "6        0.572331       6\n",
      "7        1.436785       7\n",
      "8      108.059159       8\n",
      "9        1.086983       9\n",
      "10       0.210732      10\n",
      "11       1.913730      11\n",
      "12       2.707669      12\n",
      "13       1.321347      13\n",
      "14       0.726756      14\n",
      "15       0.891552      15\n",
      "16       7.588205      16\n",
      "17       0.768536      17\n",
      "18      60.686445      18\n",
      "19       1.077899      19\n",
      "count    4.169760e+07\n",
      "mean     8.738836e+02\n",
      "std      5.293187e+04\n",
      "min      0.000000e+00\n",
      "25%      1.876570e+01\n",
      "50%      7.081913e+01\n",
      "75%      2.222231e+02\n",
      "max      1.352486e+07\n",
      "Name: meter_reading, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "########################### Predict\n",
    "#################################################################################\n",
    "if not LOCAl_TEST:\n",
    "    \n",
    "    # Load test_df from hdd\n",
    "    test_df = pd.read_pickle('test_df.pkl')\n",
    "    \n",
    "    # Remove unused columns\n",
    "    test_df = test_df[features_columns]\n",
    "    \n",
    "    # Remove test_df from hdd\n",
    "    os.system('rm test_df.pkl')\n",
    "    \n",
    "    # Read submission file\n",
    "    submission = pd.read_csv('../input/ashrae-energy-prediction/sample_submission.csv')\n",
    "\n",
    "    # Remove row_id for a while\n",
    "    del submission['row_id']\n",
    "    \n",
    "    for model_path in models:\n",
    "        print('Predictions for', model_path)\n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "        predictions = []\n",
    "        batch_size = 2000000\n",
    "        for batch in range(int(len(test_df)/batch_size)+1):\n",
    "            print('Predicting batch:', batch)\n",
    "            predictions += list(np.expm1(estimator.predict(test_df[features_columns].iloc[batch*batch_size:(batch+1)*batch_size])))\n",
    "            \n",
    "        submission['meter_reading'] += predictions\n",
    "        \n",
    "    # Average over models\n",
    "    submission['meter_reading'] /= len(models)\n",
    "    \n",
    "    # Delete test_df\n",
    "    del test_df\n",
    "     \n",
    "    # Fix negative values\n",
    "    submission['meter_reading'] = submission['meter_reading'].clip(0,None)\n",
    "\n",
    "    # Restore row_id\n",
    "    submission['row_id'] = submission.index\n",
    "    \n",
    "    ########################### Check\n",
    "    print(submission.iloc[:20])\n",
    "    print(submission['meter_reading'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "if not LOCAl_TEST:\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
